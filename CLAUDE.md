# Databricks Apps + Streamlit ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

Unity Catalogã®`samples`ã‚«ã‚¿ãƒ­ã‚°ã‚’å¯è¦–åŒ–ã™ã‚‹Streamlitãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€‚

**å¯¾è±¡ã‚«ã‚¿ãƒ­ã‚°**: `samples`(tpch, nyctaxiç­‰)

**samplesã‚«ã‚¿ãƒ­ã‚°ã‚’ä½¿ã†ç†ç”±**: 
- å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«èª­ã¿å–ã‚Šæ¨©é™ãŒã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã‚‚Service Principalæ¨©é™ã§å•é¡Œãªãã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
- ç‰¹åˆ¥ãªæ¨©é™è¨­å®šãŒä¸è¦

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

- Python 3.11
- Streamlit
- Databricks SDK (databricks-sdk)
- Databricks SQL Connector (databricks-sql-connector)

**é‡è¦: Databricks Appsã§ã¯SparkSessionã¯ä½¿ç”¨ä¸å¯**

Databricks Appsã¯ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ã®è»½é‡ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ã€Sparkãƒ‰ãƒ©ã‚¤ãƒãƒ¼/JVMãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

```python
# âŒ çµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„ - JAVA_GATEWAY_EXITEDã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MyApp").getOrCreate()

# âœ… ä»£ã‚ã‚Šã«Databricks SQL Connectorã‚’ä½¿ç”¨
from databricks import sql
from databricks.sdk.core import Config
```

ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã«ã¯å¿…ãšSQL WarehouseçµŒç”±ã®`databricks-sql-connector`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

## é–‹ç™ºã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ(åˆå›ã¯--prepare-environmentã§ä»®æƒ³ç’°å¢ƒã‚’è‡ªå‹•ä½œæˆ)
databricks apps run-local --prepare-environment

# ç’°å¢ƒå¤‰æ•°ã‚’è¿½åŠ ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ(valueFromã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§è§£æ±ºã§ããªã„ãŸã‚--envã§æ¸¡ã™)
databricks apps run-local --prepare-environment --env DATABRICKS_WAREHOUSE_ID=xxx

# ãƒ­ã‚°ç¢ºèª
databricks apps logs <app-name>
```

**é‡è¦**: 
- `--prepare-environment`ã‚’ä»˜ã‘ãªã„ã¨ã€streamlitç­‰ã®ä¾å­˜é–¢ä¿‚ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„çŠ¶æ…‹ã§å®Ÿè¡Œã•ã‚Œã€`executable file not found`ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
- app.yamlã§`valueFrom`ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ç’°å¢ƒå¤‰æ•°ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã«`--env`ãƒ•ãƒ©ã‚°ã§æ˜ç¤ºçš„ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚‹

## ãƒ‡ãƒ—ãƒ­ã‚¤(Databricks Asset Bundles)

ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯Databricks Asset Bundlesã‚’ä½¿ç”¨ã™ã‚‹ã€‚ãƒªã‚½ãƒ¼ã‚¹è¨­å®š(SQL Warehouseç­‰)ã‚‚ã‚³ãƒ¼ãƒ‰ã§ç®¡ç†ã§ãã‚‹ã€‚

### databricks.yml ã®ä¾‹

```yaml
bundle:
  name: uc-browser

resources:
  apps:
    uc-browser:
      name: uc-browser-<your-name>
      source_code_path: .
      resources:
        - name: sql-warehouse
          sql_warehouse:
            id: <warehouse-id>
            permission: CAN_USE

variables:
  warehouse_id:
    description: SQL Warehouse ID
    default: <your-warehouse-id>
```

### ãƒ‡ãƒ—ãƒ­ã‚¤ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ‡ãƒ—ãƒ­ã‚¤(åˆå›ã¯ã‚¢ãƒ—ãƒªä½œæˆã‚‚å«ã‚€)
databricks bundle deploy

# ã‚¢ãƒ—ãƒªã®çŠ¶æ…‹ç¢ºèª
databricks apps describe <app-name>

# ã‚¢ãƒ—ãƒªã®URLç¢ºèª
databricks apps get <app-name> --output json | jq -r '.url'
```

**Bundlesã®ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒªã‚½ãƒ¼ã‚¹è¨­å®š(SQL Warehouseç­‰)ã‚’ã‚³ãƒ¼ãƒ‰ã§ç®¡ç†
- UIã§ã®æ‰‹å‹•è¨­å®šãŒä¸è¦
- ç’°å¢ƒé–“(dev/staging/prod)ã®åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“

## app.yaml ã®ãƒ«ãƒ¼ãƒ«

### command ã¯å¿…ãšãƒªã‚¹ãƒˆå½¢å¼(ãƒãƒ¼ãƒˆ/ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®šç¦æ­¢)

```yaml
# âœ… æ­£ã—ã„
command:
  - streamlit
  - run
  - app.py

# âŒ é–“é•ã„: æ–‡å­—åˆ—å½¢å¼
command: "streamlit run app.py"

# âŒ é–“é•ã„: ãƒãƒ¼ãƒˆ/ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®š(Databricks Appsç’°å¢ƒã§ã¯è‡ªå‹•è¨­å®šã•ã‚Œã‚‹)
command:
  - streamlit
  - run
  - app.py
  - --server.port=8000      # æŒ‡å®šç¦æ­¢
  - --server.address=0.0.0.0  # æŒ‡å®šç¦æ­¢
```

**é‡è¦**: Databricks Appsç’°å¢ƒã§ã¯ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè‡ªå‹•è¨­å®šã•ã‚Œã€**ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç¦æ­¢**:
- `STREAMLIT_SERVER_PORT`: `DATABRICKS_APP_PORT`ã«è¨­å®š
- `STREAMLIT_SERVER_ADDRESS`: `0.0.0.0`ã«è¨­å®š

ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã¯`databricks apps run-local`ãŒãƒãƒ¼ãƒˆ8000ã§è‡ªå‹•èµ·å‹•ã™ã‚‹ãŸã‚ã€app.yamlã§ã®ãƒãƒ¼ãƒˆæŒ‡å®šã¯ä¸è¦ã€‚

### env ã¯ name/value ãƒšã‚¢ã®ãƒªã‚¹ãƒˆ

```yaml
# âœ… æ­£ã—ã„
env:
  - name: DATABRICKS_WAREHOUSE_ID
    valueFrom: sql-warehouse
  - name: LOG_LEVEL
    value: INFO

# âŒ é–“é•ã„
env:
  DATABRICKS_WAREHOUSE_ID: "xxx"
```

### valueFrom vs value ã®ä½¿ã„åˆ†ã‘

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ç”¨é€” |
|----------|------|
| `valueFrom: sql-warehouse` | SQL Warehouse ID |
| `valueFrom: serving-endpoint-name` | Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå |
| `value: "å›ºå®šå€¤"` | å›ºå®šã®è¨­å®šå€¤ |

**é‡è¦**: `valueFrom`ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€Databricks Apps UIã§ãƒªã‚½ãƒ¼ã‚¹ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
Compute > Apps > ã‚¢ãƒ—ãƒªå > Settings > Resources ã§ SQL Warehouseç­‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
ãƒªã‚½ãƒ¼ã‚¹è¨­å®šãªã—ã§ã¯`valueFrom`ãŒè§£æ±ºã•ã‚Œãšã€ã‚¢ãƒ—ãƒªãŒå‹•ä½œã—ã¾ã›ã‚“ã€‚

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶

### SQLã‚¯ã‚¨ãƒªã¯å¿…ãšãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–

```python
# âœ… æ­£ã—ã„
cursor.execute(
    "SELECT * FROM catalog.schema.table WHERE id = :id",
    {"id": user_input}
)

# âŒ é–“é•ã„ï¼ˆSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å±é™ºï¼‰
cursor.execute(f"SELECT * FROM table WHERE id = {user_input}")
```

### èªè¨¼æƒ…å ±ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã—ãªã„

```python
# âœ… æ­£ã—ã„ - SDKã®è‡ªå‹•èªè¨¼ã‚’ä½¿ç”¨
from databricks.sdk import WorkspaceClient
w = WorkspaceClient()

# âŒ é–“é•ã„
token = "dapi_xxxxxxxxxxxxx"
```

### databricks-sql-connectorã®èªè¨¼(é‡è¦)

**Databricks Appsç’°å¢ƒã§ã®æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³**:

```python
from databricks import sql
from databricks.sdk.core import Config
import os

@st.cache_resource
def get_sql_connection():
    cfg = Config()  # WorkspaceClientã§ã¯ãªãConfigã‚’ä½¿ã†
    return sql.connect(
        server_hostname=cfg.host,
        http_path=f"/sql/1.0/warehouses/{os.getenv('DATABRICKS_WAREHOUSE_ID')}",
        credentials_provider=lambda: cfg.authenticate,  # lambdaã§ãƒ©ãƒƒãƒ—å¿…é ˆ
    )
```

**ã‚ˆãã‚ã‚‹é–“é•ã„**:

```python
# âŒ é–“é•ã„1: WorkspaceClient.config.authenticateã‚’ç›´æ¥æ¸¡ã™
w = WorkspaceClient()
conn = sql.connect(
    server_hostname=w.config.host,
    credentials_provider=w.config.authenticate,  # ã‚¨ãƒ©ãƒ¼: 'dict' object is not callable
)

# âŒ é–“é•ã„2: lambdaãªã—ã§æ¸¡ã™
cfg = Config()
conn = sql.connect(
    credentials_provider=cfg.authenticate,  # ã‚¨ãƒ©ãƒ¼: 'NoneType' object is not callable
)

# âœ… æ­£ã—ã„: Configã‚’ä½¿ã„ã€lambdaã§ãƒ©ãƒƒãƒ—ã™ã‚‹
cfg = Config()
conn = sql.connect(
    server_hostname=cfg.host,
    http_path=f"/sql/1.0/warehouses/{os.getenv('DATABRICKS_WAREHOUSE_ID')}",
    credentials_provider=lambda: cfg.authenticate,
)
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `WorkspaceClient`ã§ã¯ãªã`databricks.sdk.core.Config`ã‚’ä½¿ç”¨
- `credentials_provider`ã«ã¯`lambda: cfg.authenticate`ã®å½¢å¼ã§æ¸¡ã™(lambdaã§ãƒ©ãƒƒãƒ—å¿…é ˆ)
- `cfg.authenticate`ã¯å‘¼ã³å‡ºã—æ™‚ã«ãƒ˜ãƒƒãƒ€ãƒ¼è¾æ›¸ã‚’è¿”ã™ãƒ¡ã‚½ãƒƒãƒ‰

## Unity Catalogã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

### ã‚«ã‚¿ãƒ­ã‚°/ã‚¹ã‚­ãƒ¼ãƒ/ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã®å–å¾—

Unity Catalogã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«ã¯`WorkspaceClient`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from databricks.sdk import WorkspaceClient
import streamlit as st

@st.cache_resource
def get_workspace_client():
    return WorkspaceClient()

def get_catalogs():
    w = get_workspace_client()
    return [c.name for c in w.catalogs.list()]

def get_schemas(catalog_name):
    w = get_workspace_client()
    return [s.name for s in w.schemas.list(catalog_name=catalog_name)]

def get_tables(catalog_name, schema_name):
    w = get_workspace_client()
    return [t.name for t in w.tables.list(catalog_name=catalog_name, schema_name=schema_name)]
```

### ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—

ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«ã¯`databricks-sql-connector`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from databricks import sql
from databricks.sdk.core import Config
import os
import streamlit as st

@st.cache_resource
def get_sql_connection():
    cfg = Config()
    return sql.connect(
        server_hostname=cfg.host,
        http_path=f"/sql/1.0/warehouses/{os.getenv('DATABRICKS_WAREHOUSE_ID')}",
        credentials_provider=lambda: cfg.authenticate,
    )

def get_table_data(catalog, schema, table, limit=100):
    conn = get_sql_connection()
    with conn.cursor() as cursor:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªã¯è­˜åˆ¥å­ã«ã¯ä½¿ãˆãªã„ãŸã‚ã€è¨±å¯ãƒªã‚¹ãƒˆã§æ¤œè¨¼
        query = f"SELECT * FROM `{catalog}`.`{schema}`.`{table}` LIMIT {limit}"
        cursor.execute(query)
        return cursor.fetchall_arrow().to_pandas()
```

**æ³¨æ„**: ãƒ†ãƒ¼ãƒ–ãƒ«åãªã©ã®è­˜åˆ¥å­ã¯SQLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã§ããªã„ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ãã®ã¾ã¾ä½¿ã†å ´åˆã¯è¨±å¯ãƒªã‚¹ãƒˆã§æ¤œè¨¼ã™ã‚‹ã‹ã€é¸æŠå¼UIã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

## Streamlit ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### st.set_page_config() ã¯æœ€åˆã«å‘¼ã³å‡ºã™

```python
import streamlit as st

# å¿…ãšæœ€åˆã«å‘¼ã³å‡ºã™
st.set_page_config(
    page_title="UC Browser",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ä»–ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚„ã‚³ãƒ¼ãƒ‰ã¯ã“ã®å¾Œ
```

### æ¥ç¶šã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹

```python
from databricks.sdk import WorkspaceClient
from databricks.sdk.core import Config
from databricks import sql
import os

@st.cache_resource
def get_workspace_client():
    return WorkspaceClient()

@st.cache_resource
def get_sql_connection():
    cfg = Config()
    return sql.connect(
        server_hostname=cfg.host,
        http_path=f"/sql/1.0/warehouses/{os.getenv('DATABRICKS_WAREHOUSE_ID')}",
        credentials_provider=lambda: cfg.authenticate,
    )
```

## requirements.txt ã®ãƒ«ãƒ¼ãƒ«

ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¿…ãšå›ºå®šã™ã‚‹:

```txt
streamlit==1.45.0
databricks-sdk==0.55.0
databricks-sql-connector==4.0.0
pandas==2.2.3
numpy>=1.26.0,<2.0.0
```

## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
project/
â”œâ”€â”€ CLAUDE.md           # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ databricks.yml      # Databricks Asset Bundlesè¨­å®š(ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã«ç”Ÿæˆ)
â”œâ”€â”€ app.yaml            # Databricks Appsè¨­å®š
â”œâ”€â”€ app.py              # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”œâ”€â”€ requirements.txt    # ä¾å­˜é–¢ä¿‚
â””â”€â”€ README.md           # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆèª¬æ˜
```

## ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | å¯¾å‡¦æ³• |
|-------|------|--------|
| `JAVA_GATEWAY_EXITED` | SparkSessionã‚’ä½¿ç”¨ã—ã‚ˆã†ã¨ã—ãŸ | Databricks Appsã§ã¯Sparkã¯ä½¿ç”¨ä¸å¯ã€‚SQL Connectorã‚’ä½¿ç”¨ |
| `streamlit: executable file not found` | ä¾å­˜é–¢ä¿‚æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `--prepare-environment`ã‚’ä»˜ã‘ã¦å®Ÿè¡Œ |
| `valueFrom property and can't be resolved locally` | valueFromã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§è§£æ±ºä¸å¯ | `--env VAR_NAME=value`ã§ç’°å¢ƒå¤‰æ•°ã‚’æ¸¡ã™ |
| `'dict' object is not callable` | SQL Connectorã®èªè¨¼è¨­å®šãƒŸã‚¹ | `credentials_provider=lambda: cfg.authenticate`ã®å½¢å¼ã§æ¸¡ã™ |
| `'NoneType' object is not callable` | SQL Connectorã®èªè¨¼è¨­å®šãƒŸã‚¹ | `Config()`ã‚’ä½¿ã„ã€lambdaã§ãƒ©ãƒƒãƒ—ã™ã‚‹ |
| `YAML parse error` | app.yamlã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ | command/envã®å½¢å¼ã‚’ç¢ºèª |
| `ModuleNotFoundError` | ä¾å­˜é–¢ä¿‚ä¸è¶³ | requirements.txtã‚’ç¢ºèª |
| `Connection refused` (ãƒ­ãƒ¼ã‚«ãƒ«) | ã‚¢ãƒ—ãƒªãŒèµ·å‹•ã—ã¦ã„ãªã„ | `databricks apps run-local`ã‚’å®Ÿè¡Œ |
| `401 Unauthorized` | èªè¨¼ã‚¨ãƒ©ãƒ¼ | SDKè‡ªå‹•èªè¨¼ã®è¨­å®šã‚’ç¢ºèª |
| `App deployment failed` (ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£) | 10MBè¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ | å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã€.gitignoreã‚’ç¢ºèª |
| ã‚¢ãƒ—ãƒªãŒèµ·å‹•ã—ãªã„(ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œ) | app.yamlã§ãƒãƒ¼ãƒˆ/ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®š | `--server.port`ã¨`--server.address`ã‚’å‰Šé™¤ |

## Databricks Apps ã®åˆ¶é™äº‹é …

| åˆ¶é™ | è©³ç´° |
|------|------|
| **SparkSessionä½¿ç”¨ä¸å¯** | ã‚³ãƒ³ãƒ†ãƒŠã«Sparkãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãªã—ã€‚SQL Connectorå¿…é ˆ |
| **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º** | 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Š10MBä»¥ä¸‹ |
| **èªè¨¼** | Databricksãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼å¿…é ˆ(åŒ¿åã‚¢ã‚¯ã‚»ã‚¹ä¸å¯) |
| **ãƒãƒ¼ãƒˆ/ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®šç¦æ­¢** | `STREAMLIT_SERVER_PORT`ã¨`STREAMLIT_SERVER_ADDRESS`ã¯è‡ªå‹•è¨­å®šã€‚app.yamlã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç¦æ­¢ |
| **æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ** | SQL WarehouseãŒã‚¢ã‚¤ãƒ‰ãƒ«åœæ­¢ã™ã‚‹ã¨æ¥ç¶šãŒåˆ‡ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š |
